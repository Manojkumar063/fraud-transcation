{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF6z+VvkbT6aF9l8uTEC/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manojkumar063/fraud-transcation/blob/main/fraud_transactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53BVfcIqQeOf",
        "outputId": "63a24f08-3339-4b5a-cf4c-d3141f8e4819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Amount       Time  Class\n",
            "0  375.165579   8.967380    0.0\n",
            "1  950.763592   7.989890    0.0\n",
            "2  732.261948   4.227694    0.0\n",
            "3  599.059826  14.574400    0.0\n",
            "4  156.862622  11.438980    1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of samples\n",
        "num_samples = 10000\n",
        "\n",
        "# Generate random 'Amount' and 'Time' features\n",
        "amounts = np.random.uniform(low=1, high=1000, size=num_samples)\n",
        "times = np.random.uniform(low=0, high=24, size=num_samples)\n",
        "\n",
        "# Create a balanced distribution for the 'Class' variable\n",
        "fraud_indices = np.random.choice(num_samples, size=num_samples // 20, replace=False)\n",
        "class_labels = np.zeros(num_samples)\n",
        "class_labels[fraud_indices] = 1\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Amount': amounts,\n",
        "    'Time': times,\n",
        "    'Class': class_labels\n",
        "})\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "data.to_csv('fraud_detection_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KW0OMgERxnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create a synthetic fraud detection dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=10000,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_redundant=2,\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.95],\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Add columns names\n",
        "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
        "data = pd.DataFrame(X, columns=feature_names)\n",
        "data['Class'] = y\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "data.to_csv('/content/fraud_detection_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk4Yg67xRyyR",
        "outputId": "dde37b54-791e-45e4-8e1b-febc943d48f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
            "0   1.709192   1.179545  -0.850920   1.750773  -5.184980  -1.856230   \n",
            "1   1.486885  -0.400909   1.852473   3.074283  -4.476158  -0.227699   \n",
            "2  -1.968035   1.383723  -3.953401  -1.631949   3.940087   0.464082   \n",
            "3  -2.311419   0.391472  -2.093389  -2.800227   2.672854   2.690298   \n",
            "4  -0.813161   1.240635  -0.780868  -0.506012   0.960602   0.110177   \n",
            "\n",
            "   Feature_6  Feature_7  Feature_8  Feature_9  Class  \n",
            "0   1.724183  -3.129711   0.503606   2.318989      0  \n",
            "1  -1.015016   2.697117  -0.065749   4.120415      0  \n",
            "2  -0.880014   0.255723  -2.070350  -2.091797      0  \n",
            "3  -0.295322  -0.064424   0.227488  -0.696111      0  \n",
            "4   0.323789  -0.673712  -0.043679   0.074507      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the dataset (replace 'your_dataset.csv' with your actual dataset)\n",
        "df = pd.read_csv('/content/fraud_detection_dataset.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# (Assuming the dataset has columns like 'Amount', 'Time', and 'Class')\n",
        "# Add any additional preprocessing steps based on your specific dataset\n",
        "\n",
        "# Feature engineering\n",
        "# (Assuming 'Amount' and 'Time' are relevant features; add more as needed)\n",
        "features = ['Amount', 'Time']\n",
        "X = df[features]\n",
        "y = df['Class']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model development using XGBoost\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Model evaluation on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "# Cross-validation to assess model performance\n",
        "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "print(f'Cross-Validation ROC AUC: {cv_scores.mean():.4f}')\n",
        "\n",
        "# Data preprocessing\n",
        "# (Assuming the dataset has columns like 'Amount', 'Time', and 'Class')\n",
        "# Add any additional preprocessing steps based on your specific dataset\n",
        "\n",
        "# Feature engineering\n",
        "# (Assuming 'Amount' and 'Time' are relevant features; add more as needed)\n",
        "features = ['Amount', 'Time']\n",
        "X = df[features]\n",
        "y = df['Class']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model development using XGBoost\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Model evaluation on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "# Cross-validation to assess model performance\n",
        "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "print(f'Cross-Validation ROC AUC: {cv_scores.mean():.4f}')\n",
        "\n",
        "# Data preprocessing\n",
        "# (Assuming the dataset has columns like 'Amount', 'Time', and 'Class')\n",
        "# Add any additional preprocessing steps based on your specific dataset\n",
        "\n",
        "# Feature engineering\n",
        "# (Assuming 'Amount' and 'Time' are relevant features; add more as needed)\n",
        "features = ['Amount', 'Time']\n",
        "X = df[features]\n",
        "y = df['Class']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model development using XGBoost\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Model evaluation on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC: {roc_auc:.4f}')\n",
        "\n",
        "# Cross-validation to assess model performance\n",
        "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "print(f'Cross-Validation ROC AUC: {cv_scores.mean():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cswb7s5gQonj",
        "outputId": "400e5a4a-0b40-4ed6-fef2-dfa536a150c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9435\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "ROC AUC: 0.4984\n",
            "Cross-Validation ROC AUC: 0.5152\n",
            "Accuracy: 0.9425\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "ROC AUC: 0.4979\n",
            "Cross-Validation ROC AUC: 0.5153\n",
            "Accuracy: 0.9425\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "ROC AUC: 0.4979\n",
            "Cross-Validation ROC AUC: 0.5152\n"
          ]
        }
      ]
    }
  ]
}